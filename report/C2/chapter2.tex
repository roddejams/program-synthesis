\chapter{Background}

\section{Answer Set Programming}

\subsection{The Stable Model Semantics}

The Stable Model Semantics provides a natural semantics for both normal and extended logic programs, in which instead of giving individual solutions to queries we define a set of "Answer Sets" (Stable Models) of the program. For example, given the program:

\begin{align*}
p \gets \textsf{not} \: q. \\
q \gets \textsf{not} \: p. 
\end{align*}

Because the value of p depends on the value of q, when you query Prolog for p? this program results in an infinite search, alternately searching for p and q until cancelled. However, using the Stable Model Semantics, this particular program has an answer set for each solution : \{p\} and \{q\}.

\subsubsection{Grounding}

Before you can solve an extended logic program using Answer Sets you must first ground it. To calculate the grounding of a program P you replace each rule in P by every ground instance of that rule. For example, program 

\begin{align*}
&p(1,2). \\
&q(X) \gets p(X, Y), \: \textsf{not} \: q(Y).
\end{align*}

has grounding

\begin{align*}
&p(1,2). \\
&q(1) \gets p(1, 1), \: \textsf{not} \: q(1). \\
&q(1) \gets p(1, 2), \: \textsf{not} \: q(2). \\
&q(2) \gets p(2, 1), \: \textsf{not} \: q(1). \\
&q(2) \gets p(2, 2), \: \textsf{not} \: q(2).
\end{align*}

Typically, a program containing functions has an infinite grounding. For example, 

\begin{align*}
&q(0, f(0)). \\
&p(X) \gets q(X,Y), \: \textsf{not} \: p(Y).
\end{align*}

This program would have an infinite grounding as there are an infinite amount of atoms 0, f(0), f(f(0)).. and so on. ASP solvers deal with this issue by incrementally calculating the grounding, by only generating rules such that for each atom A in the positive literals of the rule there exists another rule A as the head. This would give program 1.10 the grounding :

\begin{align*}
&q(0, f(0)). \\
&p(0) \gets q(0, f(0)), \: \textsf{not} \: p(f(0)).
\end{align*}

\subsubsection{Safety}

Because of the need for ASP solvers to ground the entire program, they are restricted to safe rules. A rule is safe if every variable in that rule occurs positively in the body of the rule. Some examples of unsafe rules are:

\begin{align*}
p(X) &\gets q(Y). \\
p(Z) &\gets \: \textsf{not} \: q(X,Y), \: r(X,Y). \\
p(X) &\gets q(X), \: \textsf{not} \: r(Y).
\end{align*}

\subsubsection{Least Herbrand Models}

Each normal logic program P has a Herbrand Base, the set of all ground atoms of the program. The program can have a Herbrand Interpretation which assigns each atom in its base a value of true or false, typically written as the set of all atoms which have  been assigned true. Then, a Herbrand Model M of P is a Herbrand Interpretation where if a rule in P has its body satisfied by M then its head must also be satisfied by M. \\ \\
A Herbrand Model M is minimal if no subset of M is also a model. For definite logic programs this model is unique and called the least Herbrand Model, however this does not always hold for normal or extended logic programs.

For example, the program 

\begin{align*}
p \gets \textsf{not} \: q. \\
q \gets \textsf{not} \: p. 
\end{align*}

has two minimal models, \{p\} and \{q\}.

\subsection{Calculating Answer Sets}

\subsubsection{Reduct}

Let P be a ground normal logic program, and X be a set of atoms. The reduct of P, P\textsuperscript{X} is calculated from P by :

\begin{enumerate}
\item Delete any rule from P which contains the negation as failure of an atom in X.
\item Delete any negation as failure atoms from the remaining rules in P.
\end{enumerate}

We then say X is an Answer Set (Stable Model) of P if it is the least Herbrand Model of  P\textsuperscript{X}. For example, if program P is:

\begin{align*}
&p(1,2). \\
&q(1) \gets p(1, 1), \: \textsf{not} \: q(1). \\
&q(1) \gets p(1, 2), \: \textsf{not} \: q(2). \\
&q(2) \gets p(2, 1), \: \textsf{not} \: q(1). \\
&q(2) \gets p(2, 2), \: \textsf{not} \: q(2).
\end{align*}

Then, when trying X = \{p(1,2), q(1)\}, the reduct is:

\begin{align*}
&p(1,2). \\
&q(1) \gets p(1, 2). \\
&q(2) \gets p(2, 2).
\end{align*}

The Least Herbrand Model M(P\textsuperscript{X}) = \{p(1,2), q(1)\}, so X is an Answer Set of P. \\ \\

It is important to note that Answer Sets are not unique and some programs might have no Answer Sets.

\subsubsection{Constraints}

Constraints are a way of filtering out unwanted Answer Sets. They are written as rules with an empty head, which is equivalent to $\bot$. Semantically, this means that any model which satisfies the body of the constraint cannot be an Answer Set. For example, the program with constraint

\begin{align*}
p \gets \textsf{not} \: q. \\
q \gets \textsf{not} \: p. \\
\gets p, \: \textsf{not} \: q.
\end{align*}

Has only one Answer Set, \{q\}.

\subsubsection{Aggregates and Choice Rules}

An aggregate is a ASP atom with the format $a \: \textsf{op} \: [h_1=w_1, h_2=w_2, \dots, h_n=w_n 	] \: b$. An aggregate is satisfied by an interpretation X if operation op applied to the multiset of weights of true literals is between the upper and lower bounds $a$ and $b$. \\ \\

I will be mainly using one application of aggregates, the choice rule. A choice rule has an aggregate using the count operation as the head of the rule, and semantically represents a choice of a number of head atoms, between the upper and lower bound. For example, the choice rule $1 \: \{ value(Coin, heads), value(Coin, tails)\} \: 1 \gets coin(Coin).$ represents that a coin can either have value heads or tails but not both.

To calculate the Answer Sets of a program P which contains aggregates by adding an additional step to the construction of the reduct. For each rule with an aggregate:

\begin{enumerate}
\item If the aggregate is not satisfied by X then convert the rule into a constraint by removing the aggregate, replacing it with $\bot$.
\item If the aggregate is satisfied by X then generate one rule for each atom A in the aggregate which is also in X, with A as the head.
\end{enumerate}

For example, consider program P with X = \{p, q, r\}:

\begin{align*}
1 \: \{p, q\} \: 2 \gets r. \\
r.
\end{align*}

the reduct is then

\begin{align*}
p \gets r. \\
q \gets r. \\
r.
\end{align*}

\subsubsection{Optimisation Statements}

An Optimisation Statement is an ASP atom of the form $ \#op \: [l_1 = w_1, l_2 = w_2, \dots, l_n = w_n]$, where $\#op$ is either $\#maximise$ or $\#minimise$, and each $l_n$ is a ground literal assigned a weight $w_n$. They allow for an ASP solver to search for optimal Answer Sets which maximise or minimise the sum of the atoms with regards to their weights. For instance, 

\begin{align*}
p \gets \textsf{not} \: q. \\
q \gets \textsf{not} \: p. \\
\#minimise \: [p = 1, q = 2]
\end{align*}

Has one optimal answer set, \{p\}.

\subsection{Learning from Answer Sets}

I will first introduce the idea of Brave and Cautious Entailment:

\begin{itemize}
\item We say an atom A is Bravely entailed by a program P if it is true in at least one Answer Set of P.
\item We say an atom A is Cautiously entailed by a program P if it is true in all Answer Sets of P.
\end{itemize}

\subsubsection{Cautious Induction}

A Cautious Induction task is defined as a search for Hypothesis $H$, given a background knowledge $B$ (an ASP program) and sets of positive and negative examples (atoms) $E^+$ and $E^-$. \\

We want to find $H$ such that:

\begin{itemize}
\item $B \cup H$ has at least one Answer Set
\item For all Answer Sets A of $H$ :
\begin{itemize}
\item $\forall e \in E^+ \: : \: e \in A$
\item $\forall e \in E^- \: : \: e \not \in A$
\end{itemize}
\end{itemize}

\subsubsection{Brave Induction}

Brave Induction is defined similarly to Cautious Induction. Given a background knowledge $B$ and sets of examples $E^+$ and $E^-$, we want to find a Hypothesis H such that, there is at least one Answer Set A of $H$ that:

\begin{itemize}
\item $\forall e \in E^+ \: : \: e \in A$
\item $\forall e \in E^- \: : \: e \not \in A$
\end{itemize}

For example, consider Program P:

\begin{align*}
p &\gets \: \textsf{not} \: q. \\
q &\gets \: \textsf{not} \: p, \: \textsf{not} \: r. \\
s &\gets r.
\end{align*}

with $E^+$ = \{s\} and $E^-$ = \{q\}. \\
The Hypothesis H = \{p, s\} is both a Brave and Cautious Inductive Solution, as there is only one Answer Set, and it satisfies all of the examples.\\

The Hypothesis H = \{s\} is a Brave Inductive Solution but not a Cautious one. It has two Answer Sets, \{p, s\} and \{q, s\}, but only \{p, s\} satisfies the examples.

\subsubsection{ASPAL}

ASPAL is an ASP algorithm which computes the solution of a Brave Inductive task by encoding it as an ASP program which has the solutions as the Answer Sets of the program.\\

First, we encode the Hypothesis space by generating a number of skeleton rules. Each skeleton rule represents a possible rule in the Hypothesis (with constant terms replaced with variables), together with an identifier. The atom rule(ID, $c_1, \dots, c_n$) represents the choice of skeleton rule with ID and the constant variables replaced by various $c_n$. The goal of the task is to find these rule atoms.\\

We next rule out Answer Sets which do not fit the examples by adding a goal rule and a constraint, of the format :

\begin{align*}
&\textsf{goal} \gets e_1^+, \dots, e_n^+, \: \textsf{not} \: e_1^-, \dots, \: \textsf{not} \: e_n^- \\
&\gets \textsf{not goal}
\end{align*}

This rules out any answer set which does not contain all of the positive examples and contains any negative examples.

We then make sure the Answer Sets of the program correspond to the optimal solutions of the task by adding an optimisation statement
\begin{align*}
\# minimise \: [rule(ID, c_1, \dots, c_n) = ruleLength, \dots].
\end{align*}

where there is a rule predicate for each skeleton rule, weighted by the length of that rule.

As an example of the entire ASPAL encoding, consider the program :

\begin{align*}
bird(a).& \: &bird(b).\\
ability(fly).& \: &ability(swim). \\
can(a, fly).& \: &can(b, swim).
\end{align*}

with mode declarations 

\begin{align*}
&modeh(penguin(+bird)).\\
&modeb(\textsf{not} \: can(+bird, \#ability)).
\end{align*}

and examples $E^+$ = \{penguin(b)\} and $E^-$ = \{penguin(a)\}. \\
This task has encoding:

\begin{align*}
&penguin(V1) \gets bird(V1), \: rule(1).\\
&penguin(V1) \gets bird(V1), \: \textsf{not} \: can(V1, C1), rule(2, C1).\\
&penguin(V1) \gets bird(V1), \: \textsf{not} \: can(V1, C1), \: \textsf{not} \: can(V1, C2), rule(3, C1, C2).\\
&\%\%\%\%\%\%\\
&\{rule(1), rule(2, fly), rule(2, swim), rule(3, fly, swim)\}.\\
&\#minimise \: [rule(1)=1, rule(2, fly) = 2, rule(2, swim) = 2, rule(3, fly, swim) = 3]. \\
&\%\%\%\%\%\%\\
&goal \gets pengiun(b), \: \textsf{not} \: penguin(a).\\
&\gets \: \textsf{not} \: goal.
\end{align*}

\subsection{ASP Solvers}

\subsubsection{CLASP}

\section{Inductive Functional Programming}

\begin{itemize}
\item Inductive Functional Programming is the automatic synthesis of program
\item Applications
\item Traditionally, there have been two approaches to IFP. The analytical approach performs pattern matching on the given examples, usually performing a two-step process of first generalising the examples and then folding this generalisation into a recursive program. The "generate and test" search approach works by generating an infinite stream of candidate programs and then test these candidates to see if they correctly model the examples. \\ \\

Each approach has its own advantages and disadvantages. The analytical approach, while fast, can be very limited in its target language and the types of programs it can generate, typically being limited to reasoning about data structures such as lists or trees. On the other hand, the search based approach is a lot less restricted but has worse performance due to the potentially huge search space. This issue is minimised by various optimisations performed on the generated programs, typically using a subset of the examples as an initial starting point.
\item Overview of current approaches and how they relate 
\end{itemize}


\section{Haskell}

\pagebreak
\renewcommand\bibname{{References}}
\bibliography{References}
\bibliographystyle{plain}