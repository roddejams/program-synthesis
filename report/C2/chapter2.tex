\chapter{Background}

\section{Answer Set Programming}

\subsection{The Stable Model Semantics}

The Stable Model Semantics provides a natural semantics for both normal and extended logic programs, in which instead of giving individual solutions to queries we define a set of "Answer Sets" (Stable Models) of the program. For example, given the program:

\begin{lstlisting}
p :- not q.
q :- not p.
\end{lstlisting}

The value of p depends on the value of q, when you query Prolog for p? this program results in an infinite search, alternately searching for p and q until cancelled. However, using the Stable Model Semantics, this particular program has an answer set for each solution : \{p\} and \{q\}.

\subsubsection{Grounding}

Before you can solve an extended logic program using Answer Sets you must first ground it. To calculate the grounding of a program P you replace each rule in P by every ground instance of that rule. For example, program 

\begin{lstlisting}
p(1, 2).
q(X) :- p(X, Y), not q(Y).
\end{lstlisting}

has grounding

\begin{lstlisting}
p(1,2).
q(1) :- p(1, 1), not q(1).
q(1) :- p(1, 2), not q(2).
q(2) :- p(2, 1), not q(1).
q(2) :- p(2, 2), not q(2).
\end{lstlisting}

Typically, a program containing functions has an infinite grounding. For example, 

\begin{lstlisting}
q(0, f(0)).
p(X) :- q(X, Y), not p(Y).
\end{lstlisting}

This program would have an infinite grounding as there are an infinite amount of atoms 0, f(0), f(f(0)).. and so on. ASP solvers deal with this issue by incrementally calculating the grounding, by only generating rules such that for each atom A in the positive literals of the rule there exists another rule A as the head. This would give program 1.10 the grounding :

\begin{lstlisting}
q(0, f(0)).
p(0) :- q(0, f(0)), not p(f(0)).
\end{lstlisting}

\subsubsection{Safety}

Because of the need for ASP solvers to ground the entire program, they are restricted to safe rules. A rule is safe if every variable in that rule occurs positively in the body of the rule. Some examples of unsafe rules are:

\begin{lstlisting}
p(X) :- q(Y).
p(Z) :- not q(X, Y), r(X, Y).
p(X) :- q(X), not r(Y).
\end{lstlisting}

\subsubsection{Least Herbrand Models}

Each normal logic program P has a Herbrand Base, the set of all ground atoms of the program. The program can have a Herbrand Interpretation which assigns each atom in its base a value of true or false, typically written as the set of all atoms which have  been assigned true. Then, a Herbrand Model M of P is a Herbrand Interpretation where if a rule in P has its body satisfied by M then its head must also be satisfied by M. \\ \\
A Herbrand Model M is minimal if no subset of M is also a model. For definite logic programs this model is unique and called the least Herbrand Model, however this does not always hold for normal or extended logic programs.

For example, the program 

\begin{lstlisting}
p :- not q.
q :- not p.
\end{lstlisting}

has two minimal models, \{p\} and \{q\}.

\subsection{Calculating Answer Sets}

\subsubsection{Reduct}

Let P be a ground normal logic program, and X be a set of atoms. The reduct of P, P\textsuperscript{X} is calculated from P by :

\begin{enumerate}
\item Delete any rule from P which contains the negation as failure of an atom in X.
\item Delete any negation as failure atoms from the remaining rules in P.
\end{enumerate}

We then say X is an Answer Set (Stable Model) of P if it is a least Herbrand Model of  P\textsuperscript{X}. For example, if program P is:

\begin{lstlisting}
p(1, 2).
q(1) :- p(1, 1), not q(1).
q(1) :- p(1, 2), not q(2).
q(2) :- p(2, 1), not q(1).
q(2) :- p(2, 2), not q(2).
\end{lstlisting}

Then, when trying X = \{p(1,2), q(1)\}, the reduct is:

\begin{lstlisting}
p(1, 2).
q(1) :- p(1, 2).
q(2) :- p(2, 2).
\end{lstlisting}

A Least Herbrand Model M(P\textsuperscript{X}) = \{p(1,2), q(1)\}, so X is an Answer Set of P. \\ \\

It is important to note that Answer Sets are not unique and some programs might have no Answer Sets.

\subsubsection{Constraints}

Constraints are a way of filtering out unwanted Answer Sets. They are written as rules with an empty head, which is equivalent to $\bot$. Semantically, this means that any model which satisfies the body of the constraint cannot be an Answer Set. For example, the program with constraint

\begin{lstlisting}
p :- not q.
q :- not p.
:- p, not q.
\end{lstlisting}

Has only one Answer Set, \{q\}.

\subsubsection{Aggregates and Choice Rules}

An aggregate is a ASP atom with the format $a \: \textsf{op} \: [h_1=w_1, h_2=w_2, \dots, h_n=w_n 	] \: b$. An aggregate is satisfied by an interpretation X if operation op applied to the multiset of weights of true literals is between the upper and lower bounds $a$ and $b$. \\ \\

I will be mainly using one application of aggregates, the choice rule. A choice rule has an aggregate using the count operation as the head of the rule, and semantically represents a choice of a number of head atoms, between the upper and lower bound. For example, the choice rule $1 \: \{ value(Coin, heads), value(Coin, tails)\} \: 1 \gets coin(Coin).$ represents that a coin can either have value heads or tails but not both.

To calculate the Answer Sets of a program P which contains aggregates by adding an additional step to the construction of the reduct. For each rule with an aggregate:

\begin{enumerate}
\item If the aggregate is not satisfied by X then convert the rule into a constraint by removing the aggregate, replacing it with $\bot$.
\item If the aggregate is satisfied by X then generate one rule for each atom A in the aggregate which is also in X, with A as the head.
\end{enumerate}

For example, consider program P with X = \{p, q, r\}:

\begin{lstlisting}
1 {p, q} 2 :- r.
r.
\end{lstlisting}

the reduct is then

\begin{lstlisting}
p :- r.
q :- r.
r.
\end{lstlisting}

\subsubsection{Optimisation Statements}

An Optimisation Statement is an ASP atom of the form $ \#op \: [l_1 = w_1, l_2 = w_2, \dots, l_n = w_n]$, where $\#op$ is either $\#maximise$ or $\#minimise$, and each $l_n$ is a ground literal assigned a weight $w_n$. They allow for an ASP solver to search for optimal Answer Sets which maximise or minimise the sum of the atoms with regards to their weights. For instance, 

\begin{lstlisting}
p :- not q.
q :- not p.
#minimise [p = 1, q = 2].
\end{lstlisting}

Has one optimal answer set, \{p\}.

\subsection{Learning from Answer Sets}

I will first introduce the idea of Brave and Cautious Entailment:

\begin{itemize}
\item We say an atom A is Bravely entailed by a program P if it is true in at least one Answer Set of P.
\item We say an atom A is Cautiously entailed by a program P if it is true in all Answer Sets of P.
\end{itemize}

\subsubsection{Cautious Induction}

A Cautious Induction task is defined as a search for Hypothesis $H$, given a background knowledge $B$ (an ASP program) and sets of positive and negative examples (atoms) $E^+$ and $E^-$. \\

We want to find $H$ such that:

\begin{itemize}
\item $B \cup H$ has at least one Answer Set
\item For all Answer Sets A of $H$ :
\begin{itemize}
\item $\forall e \in E^+ \: : \: e \in A$
\item $\forall e \in E^- \: : \: e \not \in A$
\end{itemize}
\end{itemize}

\subsubsection{Brave Induction}

Brave Induction is defined similarly to Cautious Induction. Given a background knowledge $B$ and sets of examples $E^+$ and $E^-$, we want to find a Hypothesis H such that, there is at least one Answer Set A of $H$ that:

\begin{itemize}
\item $\forall e \in E^+ \: : \: e \in A$
\item $\forall e \in E^- \: : \: e \not \in A$
\end{itemize}

For example, consider the program:

\begin{lstlisting}
p :- not q.
q :- not p, not r.
s :- r.
\end{lstlisting}

with $E^+$ = \{s\} and $E^-$ = \{q\}. \\
The Hypothesis H = \{p, s\} is both a Brave and Cautious Inductive Solution, as there is only one Answer Set, and it satisfies all of the examples.\\

The Hypothesis H = \{s\} is a Brave Inductive Solution but not a Cautious one. It has two Answer Sets, \{p, s\} and \{q, s\}, but only \{p, s\} satisfies the examples.

\subsubsection{ASPAL}

ASPAL is an ASP algorithm which computes the solution of a Brave Inductive task by encoding it as an ASP program which has the solutions as the Answer Sets of the program.\\

First, we encode the Hypothesis space by generating a number of skeleton rules. Each skeleton rule represents a possible rule in the Hypothesis (with constant terms replaced with variables), together with an identifier. The atom rule(ID, $c_1, \dots, c_n$) represents the choice of skeleton rule with ID and the constant variables replaced by various $c_n$. The goal of the task is to find these rule atoms.\\

We next rule out Answer Sets which do not fit the examples by adding a goal rule and a constraint, of the format :

\begin{lstlisting}[mathescape=true]
goal :- $e_1^+, \dots, e_n^+, \: \textsf{not} \: e_1^-, \dots, \: \textsf{not} \: e_n^-$
:- not goal.
\end{lstlisting}

This rules out any answer set which does not contain all of the positive examples and contains any negative examples.

We then make sure the Answer Sets of the program correspond to the optimal solutions of the task by adding an optimisation statement
\begin{lstlisting}[mathescape=true]
#minimise [rule(ID, $c_1, \dots, c_n$) = ruleLength, $\dots$].
\end{lstlisting}

where there is a rule predicate for each skeleton rule, weighted by the length of that rule.

As an example of the entire ASPAL encoding, consider the program :

\begin{lstlisting}
bird(a).		bird(b).
ability(fly).		ability(swim).
can(a, fly).		can(b, swim).
\end{lstlisting}

with mode declarations 

\begin{lstlisting}
modeh(penguin(+bird)).
modeb(not can(+bird, #ability)).
\end{lstlisting}

and examples $E^+$ = \{penguin(b)\} and $E^-$ = \{penguin(a)\}. \\
This task has encoding:

\begin{lstlisting}
penguin(V1) :- bird(V1), rule(1).
penguin(V1) :- bird(V1), not can(V1, C1), rule(2, C1).
penguin(V1) :- bird(V1), not can(V1, C1), not can(V1, C2), rule(3, C1, C2).
%%%%%%%%
{rule(1), rule(2, fly), rule(2, swim), rule(3, fly, swim)}
#minimise [rule(1) = 1, rule(2, fly) = 2, rule(2, swim) = 2, rule(3, fly, swim) = 3].
%%%%%%%%
goal :- penguin(b), not penguin(a).
:- not goal.
\end{lstlisting}

\subsection{ASP Solvers}

Typically, ASP solvers work in two parts.

\begin{itemize}
\item First the input program must be converted into the ground finite logic program by a grounder. Typically this step also includes optimisations to help the solver.
\item The ground program is then passed into the solver which calculates the Answer Sets of the program.
\end{itemize}

The main popular ASP solvers are SMODELS, DLV and CLASP. CLASP is the ASP solver which I will be using to run my learning tasks as part of this project. It consists of three programs : The grounder \textit{gringo}, the ASP solver \textit{clasp} and the utility \textit{clingo} which combines the two.

\subsubsection{Gringo}

Gringo calculates the ground logic program by replacing the variables in the program by ground terms. The resulting program needs only be an equivalent program (meaning it has the same Answer Sets), to be able to deal with programs with infinite number of Answer Sets.

\section{Inductive Functional Programming}

Traditionally, there have been two approaches to IFP. The analytical approach performs pattern matching on the given examples, usually performing a two-step process of first generalising the examples and then folding this generalisation into a recursive program. The "generate and test" search approach works by generating an infinite stream of candidate programs and then test these candidates to see if they correctly model the examples. \\

Each approach has its own advantages and disadvantages. The analytical approach, while fast, can be very limited in its target language and the types of programs it can generate, typically being limited to reasoning about data structures such as lists or trees. On the other hand, the search based approach is a lot less restricted but has worse performance due to the potentially huge search space. This issue is minimised by various optimisations performed on the generated programs, typically using a subset of the examples as an initial starting point.

\subsection{Conditional Constructor Systems}

Programs are represented in a functional style as a \textit{term rewriting system}, i.e a set of rules of the form $l \rightarrow r$. The LHS of the rule, $l$, has the format $F(a_1, \dots, a_n)$ where $F$ is a user-defined function and $a_1$ to $a_n$ are variables or pre-defined data type functions(constructors). In addition, rules must be \textit{bound}, meaning that all variables that appear on the right hand side of a rule must also appear on the left hand side.\\

We can also extend these rules with conditionals, which must be met if the given rule is to be applied. Conditionals are of the form $l \rightarrow r \Leftarrow v_1 = u_1, \dots, v_n = u_n$, where each $v_n = u_n$ is an \textit{equality constraint} which has to hold for the rule to be applied.

Using this definition we can begin to learn TRS. We take the sets of positive and negative examples ($E^+$ and $E^-$), written as sets of input/output examples in the form of unconditional re-write rules, and a \textit{background knowledge} $BK$ which is a set of additional re-write used to help computation. \\

The goal of the learning task is then to a finite set of rewrite rules $R$ such that:

\begin{itemize}
\item $R \cup BK \models E^+$ (covers all of the positive examples).
\item $R \cup BK \not \models E^-$ (covers none of the negative examples).
\end{itemize}

Because the target domain of this task is often very large, two extra constraints are added to the type of rule we are allowed to learn.\\
Restriction Bias is similar to the language bias typically found in ILP systems, and restricts the format of the learned rules and conditionals, for example to avoid mutual recursion. \\
Preference Bias introduces an idea of optimisation to the task such that the learned rules are optimal while also satisfying the examples. Some such optimisations are preference to shorter right hand sides or preference to as few rules as possible.

\subsection{Overview of current tools}

Here I will discuss two modern IFP systems which I will use to compare to my system as part of my evaluation:

\begin{itemize}
\item \textbf{MagicHaskeller} is based on the generate and test approach to IFP, and works by generating a stream of progressively more complicated programs and then testing them against the given specification. This search is exhaustive and performed in a breadth-first manner with application of a variant of Spivey's Monad.\\

MagicHaskeller has the advantage of being easy to use as you only have to give the specification of the desired function as an argument, which is typically no more than a few lines of code in the target language. The tool also has a particularly large component library of built-in Haskell primitives which the tool uses where possible.\\

However, MagicHaskeller does not have the ability to compute large programs, due to the performance cost of exhaustive breadth-first search. This problem, however, is typical of the generate and test approach.

\item \textbf{Igor II} is based on the analytical approach is specialised towards learning recursive programs. It works by first calculating the \textit{least-general generalisation} of the examples, which extracts terms which are common between the examples and terms which are uncommon are represented as variables. If this initial hypothesis is incomplete (i.e not a correct functional program) then four refinement operators are applied to attempt to complete it:
\begin{enumerate}
\item Partition the examples into two subsets and generate a new, more specific least general generalisation for each subset. This partition is calculated using pattern matching on the variables from the lhs of the initial hypothesis.
\item If the rhs of the initial hypothesis has a function as its root element then each unbound argument of this function is treated as a subproblem, and new functions are introduced for each argument.
\item The rhs of the initial hypothesis may be replaced by a recursive call to a user-defined function, where the arguments of this call are newly introduced functions.
\item If the examples match certain properties then a higher order function may be introduced, and synthesising the arguments to this function becomes the new induction problem.
\end{enumerate}
Igor II has a number of limitations. If there are a large number of examples then performance can be impacted because of the large number of combinations when matching to a defined function. Synthesis can fail or perform incorrectly if some examples from the middle of the set are removed, meaning if we want to specify a particularly large or complex example we also have to specify all of the smaller examples, reducing performance.
\end{itemize}

\nocite{*}

\renewcommand\bibname{{References}}
\bibliography{References}
\bibliographystyle{plain}

\pagebreak