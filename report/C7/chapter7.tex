\chapter{Conclusions and Future Work}

In this chapter I will give an overview of features that would have been implemented in the learning tool given more time, and some opinions on the positives and negatives of my implementation.

\section{Future Work}

\subsection{Type Usage}
One of the main features the current version of the learning tool is missing is the ability to handle input and return types that are not integers. This adds a significant restriction on what the tool can learn, and if implemented would not adversely affect performance. \\ \\
The implementation of this feature would be based on performing analysis on the types of the examples. By using the UI to calculate the type of each input argument and the type of the example, it would be possible to evaluate which operations can be performed on those input types to return the correct output. It would then be possible to generate the set of skeleton rules based on these operations. \\ \\
As an example, consider the \lstinline{isPrime} function, which returns a true if the input to the function is a prime number, and false if not. It is important to know that the function takes two arguments - the first being the number to check the primality of, and the second being a divisor which is checked against the first argument and decremented at each recursive step. \\ \\ %{
Analysing the types of this example would tell the tool that all of the input types are integer, and that the output type is boolean. This would limit the tool to only generate skeleton rules which have bodies which return booleans - i.e having \lstinline{==}, \lstinline{>}, \lstinline{<} or the recursive call to \lstinline{isPrime} in the top level in the body expression. \\ \\
This type of implementation would not affect performance because it is not increasing the size of the skeleton rules, only changing their content. In some cases even, the number of skeleton rules would be fewer if restricted in this way, as there may be fewer in-built operations to enumerate over.

\subsection{Lists and Strings}
Another key part of the Haskell language I am missing in my potential target language is list functionality. Without lists, a large feature of functional programming is missing, reducing the overall expressiveness of the possible learning domain. \\ \\
Implementing the learning of lists would not be a trivial task, although it is already partially complete. The ASP interpreter is already expressive enough to handle lists, representing them as nested tuples. In a similar way to how the Haskell list \lstinline{[0, 1, 2, 3]} is internally represented as multiple items prepended together, \lstinline{(0 : (1 : (2 : (3 : []))))}, the list is represented in ASP as \lstinline{(0, (1, (2, (3, e))))}, where \lstinline{e} is an internal representation of the empty list. \\ \\
The interpreter handles these lists in the same way it handles multiple arguments. If it has to evaluate a tuple, it then generates \lstinline{eval} terms for call complex arguments. Similar behaviour holds for values of expressions and checking for complexity. This behaviour is covered by the following rules : \\ %{

\begin{lstlisting}
value_with((A, B), (V1, V2), Args) :-
   eval_with((A, B), Args), value_with(A, V1, Args), value_with(B, V2, Args).

eval_with(A, Args) :- complex(A), eval_with((A, B), Args).
eval_with(B, Args) :- complex(B), eval_with((A, B), Args).

check_if_complex(A) :- check_if_complex((A, B)).
check_if_complex(B) :- check_if_complex((A, B)).

complex((A, B)) :- complex(A), check_if_complex((A, B)).
complex((A, B)) :- complex(B), check_if_complex((A, B)).

n_complex((A, B)) :- n_complex(A), n_complex(B), check_if_complex((A, B)).
\end{lstlisting}
\mbox{}\\
Implementing lists for the constraint based approach would be similar. Through use of rules which define equality on tuples, it should not be difficult to maintain the equality constraints which fail when a contradiction occurs. The check for termination would work in a similar way, although some advanced implementation of match rules may be necessary to check for list patterns. \\ \\
The difficult part of lists would be enumerating all of the different possibilities. While my current implementation allows for a small range of constants to be learned as part of the skeleton rules, to allow for lists this range would have to be a lot wider. and consist of all combinations of constants up to a certain list length. \\ \\
The implementation on string operations would work in a similar way, by treating strings as list of characters. The string \lstinline{"hello"} would be represented in ASP as the list \lstinline{(h, (e, (l, (l, (o, e)))))}. This would allow all basic operations on lists to be performed on strings, and allow them to be learned in the same way.

\subsection{Expanding the Background Knowledge}
To increase the expressiveness of the learned functions, it would be beneficial to increase the number of inbuilt functions in the background knowledge, to better represent functions a person may write. The functions I have considered for addition are part of the \lstinline{Prelude.hs} standard Haskell library %{
\begin{center}
\begin{tabular}{ c | c | c }
\textbf{List Processing} & \textbf{Arithmetic} & \textbf{Tuples} \\
\hline
\begin{lstlisting}
length
\end{lstlisting} 
& 
\begin{lstlisting}
div
\end{lstlisting} 
& 
\begin{lstlisting}
zip
\end{lstlisting}
\\
\begin{lstlisting}
++
\end{lstlisting}
& 
\begin{lstlisting}
mod
\end{lstlisting}
&
\begin{lstlisting}
unzip
\end{lstlisting}
\\
\begin{lstlisting}
reverse
\end{lstlisting}
& 
\begin{lstlisting}
gcd
\end{lstlisting}
& 
\begin{lstlisting}
fst
\end{lstlisting}
\\
\begin{lstlisting}
head
\end{lstlisting}
& 
\begin{lstlisting}
even
\end{lstlisting}
& 
\begin{lstlisting}
snd
\end{lstlisting}
\\
\begin{lstlisting}
last
\end{lstlisting}
& 
\begin{lstlisting}
odd
\end{lstlisting}
& \\
\begin{lstlisting}
tail
\end{lstlisting}
&
\begin{lstlisting}
sum
\end{lstlisting}
& \\
&
\begin{lstlisting}
product
\end{lstlisting}
 & 
\end{tabular}
\end{center}
\mbox{} \\
Usage of these functions would be implemented by adding more rules to the skeleton rules. The skeleton rules would contain all possible usages of these in-built functions with the given arguments, restricted by the types of the examples. Then, the ASP interpreter would have to contain rules which handle the generating the value of expressions containing these functions. Through use of addition lua scripting, the complexity of this task could be reduced, allowing for example the calculation of a length of a list to be evaluated in one rule instead of many.\\ \\
In addition to these in-built functions, implementing higher-order functions would also increase the expressiveness of learned functions. The functions \lstinline{map}, \lstinline{fold} and \lstinline{filter} would be added to the skeleton rules in a similar way, with the addition of having to enumerate all possible functions as additional arguments. However, evaluating the higher order functions might prove to be more difficult. Applying operations to every element in a list would be difficult to evaluate using just ASP, but through use of lua scripting, again it would be possible, albeit slow. %{

\subsection{Advanced Skeleton Rule Generation}
One of the main advantages to existing Inductive Functional Programming implementations is the way they generate their search space. MagicHaskeller generates a stream of valid functions, ordered by depth, and checks each one sequentially against the examples. If my learning tool were to implement such a system then it would have the potential to greatly increase performance. Currently, my skeleton rule generation is a naive process, containing many irrelevant rules which could not return valid programs, and my implementation of optimisation means that clingo has to enumerate all possible programs before returning a result, instead of being able to return as soon as a solution is found as would occur using a stream of rules. However, it is not clear how this would be easily implemented in ASP. \\ \\
Another potential skeleton rule generation optimisation would be to be explicit with the difference between rules that can appear in the base case and the recursive cases of generated functions. Currently, all possible rules can appear in the base case of a recursive function, even complex function calls. This means that the learning task has to evaluate a large number of redundant possibilities, reducing performance. \\ \\
To avoid this, the tool would limit the possible rules for base cases to simple arithmetic functions that are only one, potentially two operations deep. Then, the recursive cases would be limited to only expressions which contain calls to functions or in-built operations. Theoretically, this optimisation could lead to around a 50\% increase in performance on functions with two cases, one base case and one recursive case, as it would greatly reduce the time taken to enumerate the base case, which in this case is half the rules needed to be learned.

\subsection{Supporting Multiple Function Calls}
The main limitation on the constraint based learning approach is that it cannot handle multiple function calls. For example, if the tool encounters the equality constraint \lstinline{eq(add(f(x - 1), f(x - 2)), 8)}, then it cannot proceed as it does not know the value of either 

\subsection{Loading known functions}

\subsection{Parallel Learning}

\section{Conclusions}

\subsection{What worked}

\subsection{Areas for improvement}

\pagebreak
%\renewcommand\bibname{{References}}
%\bibliography{References}
%\bibliographystyle{plain}