\chapter{Conclusions and Future Work}

In this chapter I will give an overview of the went well during development of the tool , and an list of features that would have been implemented in the learning tool given more time.

\section{Conclusions}
Overall, I am very pleased with the finished learner. While I did not achieve everything I set out to accomplish, I was deliberately ambitious with those goals and to achieve it all would have been overly difficult.  In addition, the tool has been designed in such a way that adding these additional features would not be overly difficult and require a large amount of extra code.

\subsection{Learning}
I am most pleased with the current version of my learning task. It details a successful new approach to Inductive Functional Programming, showing that it is possible to use ASP to learn directly recursive programs in an efficient way.\\ \\
The constraint based approach exceeded expectations, allowing a performance increase of approximately 50\%. This performance increase allows the tool to attempt learning more expressive programs than the interpreted approach, while not sacrificing performance.\\ \\
Whilst the fact that I had to implement an entire new learning approach towards the end of the project has meant that the size of the target language is smaller than planned, it would only be a matter of time before additional features would be added, rather than them not being possible to add.

\subsection{UI} 
I am also proud of the quality of the user interface I created. Usage of the bootstrap libraries has meant that the UI looks clean and functional without being overly complicated. As an added bonus, it was not difficult to implement, allowing me to easily change and modify the UI based on feedback. \\ \\
My usage of the Play Framework has also increased the usability of the tool. The learning running as a asynchronous process means that the interface feels fluid, allowing the user to change tabs and work on other functions while waiting for learning to be complete. Play has also made development of the UI easier, clearly separating the frontend and backend and keeping my code organised and fairly simple. \\ \\
One feature that works well is the generation of Haskell programs from ASP. While the output from clingo is simply an answer set containing atoms specifying the chosen rules, this is converted into valid Haskell by the UI. The UI also allows for the Haskell to be downloaded and ran locally by the user, ran by the UI itself to check the validity of the learned program. This feature makes my tool stand out from other existing systems, which have output that can be very difficult to analyse or convert into something runnable.

\section{Future Work}

\subsection{Type Usage}
One of the main features the current version of the learning tool is missing is the ability to handle input and return types that are not integers. This adds a significant restriction on what the tool can learn, and if implemented would not adversely affect performance. \\ \\
The implementation of this feature would be based on performing analysis on the types of the examples. By using the UI to calculate the type of each input argument and the type of the example, it would be possible to evaluate which operations can be performed on those input types to return the correct output. It would then be possible to generate the set of skeleton rules based on these operations. \\ \\
As an example, consider the \lstinline{isPrime} function, which returns a true if the input to the function is a prime number, and false if not. It is important to know that the function takes two arguments - the first being the number to check the primality of, and the second being a divisor which is checked against the first argument and decremented at each recursive step. \\ \\ %{
Analysing the types of this example would tell the tool that all of the input types are integer, and that the output type is boolean. This would limit the tool to only generate skeleton rules which have bodies which return booleans - i.e having \lstinline{==}, \lstinline{>}, \lstinline{<} or the recursive call to \lstinline{isPrime} in the top level in the body expression. \\ \\
This type of implementation would not affect performance because it is not increasing the size of the skeleton rules, only changing their content. In some cases even, the number of skeleton rules would be fewer if restricted in this way, as there may be fewer in-built operations to enumerate over.

\subsection{Lists and Strings}
Another key part of the Haskell language I am missing in my potential target language is list functionality. Without lists, a large feature of functional programming is missing, reducing the overall expressiveness of the possible learning domain. \\ \\
Implementing the learning of lists would not be a trivial task, although it is already partially complete. The ASP interpreter is already expressive enough to handle lists, representing them as nested tuples. In a similar way to how the Haskell list \lstinline{[0, 1, 2, 3]} is internally represented as multiple items prepended together, \lstinline{(0 : (1 : (2 : (3 : []))))}, the list is represented in ASP as \lstinline{(0, (1, (2, (3, e))))}, where \lstinline{e} is an internal representation of the empty list. \\ \\
The interpreter handles these lists in the same way it handles multiple arguments. If it has to evaluate a tuple, it then generates \lstinline{eval} terms for call complex arguments. Similar behaviour holds for values of expressions and checking for complexity. This behaviour is covered by the following rules : \\ %{

\begin{lstlisting}
value_with((A, B), (V1, V2), Args) :-
   eval_with((A, B), Args), value_with(A, V1, Args), value_with(B, V2, Args).

eval_with(A, Args) :- complex(A), eval_with((A, B), Args).
eval_with(B, Args) :- complex(B), eval_with((A, B), Args).

check_if_complex(A) :- check_if_complex((A, B)).
check_if_complex(B) :- check_if_complex((A, B)).

complex((A, B)) :- complex(A), check_if_complex((A, B)).
complex((A, B)) :- complex(B), check_if_complex((A, B)).

n_complex((A, B)) :- n_complex(A), n_complex(B), check_if_complex((A, B)).
\end{lstlisting}
\mbox{}\\
Implementing lists for the constraint based approach would be similar. Through use of rules which define equality on tuples, it should not be difficult to maintain the equality constraints which fail when a contradiction occurs. The check for termination would work in a similar way, although some advanced implementation of match rules may be necessary to check for list patterns. \\ \\
The difficult part of lists would be enumerating all of the different possibilities. While my current implementation allows for a small range of constants to be learned as part of the skeleton rules, to allow for lists this range would have to be a lot wider. and consist of all combinations of constants up to a certain list length. \\ \\
The implementation on string operations would work in a similar way, by treating strings as list of characters. The string \lstinline{"hello"} would be represented in ASP as the list \lstinline{(h, (e, (l, (l, (o, e)))))}. This would allow all basic operations on lists to be performed on strings, and allow them to be learned in the same way.

\subsection{Expanding the Background Knowledge}
To increase the expressiveness of the learned functions, it would be beneficial to increase the number of inbuilt functions in the background knowledge, to better represent functions a person may write. The functions I have considered for addition are part of the \lstinline{Prelude.hs} standard Haskell library %{
\begin{center}
\begin{tabular}{ c | c | c }
\textbf{List Processing} & \textbf{Arithmetic} & \textbf{Tuples} \\
\hline
\begin{lstlisting}
length
\end{lstlisting} 
& 
\begin{lstlisting}
div
\end{lstlisting} 
& 
\begin{lstlisting}
zip
\end{lstlisting}
\\
\begin{lstlisting}
++
\end{lstlisting}
& 
\begin{lstlisting}
mod
\end{lstlisting}
&
\begin{lstlisting}
unzip
\end{lstlisting}
\\
\begin{lstlisting}
reverse
\end{lstlisting}
& 
\begin{lstlisting}
gcd
\end{lstlisting}
& 
\begin{lstlisting}
fst
\end{lstlisting}
\\
\begin{lstlisting}
head
\end{lstlisting}
& 
\begin{lstlisting}
even
\end{lstlisting}
& 
\begin{lstlisting}
snd
\end{lstlisting}
\\
\begin{lstlisting}
last
\end{lstlisting}
& 
\begin{lstlisting}
odd
\end{lstlisting}
& \\
\begin{lstlisting}
tail
\end{lstlisting}
&
\begin{lstlisting}
sum
\end{lstlisting}
& \\
&
\begin{lstlisting}
product
\end{lstlisting}
 & 
\end{tabular}
\end{center}
\mbox{} \\
Usage of these functions would be implemented by adding more rules to the skeleton rules. The skeleton rules would contain all possible usages of these in-built functions with the given arguments, restricted by the types of the examples. Then, the ASP interpreter would have to contain rules which handle the generating the value of expressions containing these functions. Through use of addition lua scripting, the complexity of this task could be reduced, allowing for example the calculation of a length of a list to be evaluated in one rule instead of many.\\ \\
In addition to these in-built functions, implementing higher-order functions would also increase the expressiveness of learned functions. The functions \lstinline{map}, \lstinline{fold} and \lstinline{filter} would be added to the skeleton rules in a similar way, with the addition of having to enumerate all possible functions as additional arguments. However, evaluating the higher order functions might prove to be more difficult. Applying operations to every element in a list would be difficult to evaluate using just ASP, but through use of lua scripting, again it would be possible, albeit slow. %{

\subsection{Advanced Skeleton Rule Generation}
One of the main advantages to existing Inductive Functional Programming implementations is the way they generate their search space. MagicHaskeller generates a stream of valid functions, ordered by depth, and checks each one sequentially against the examples. If my learning tool were to implement such a system then it would have the potential to greatly increase performance. Currently, my skeleton rule generation is a naive process, containing many irrelevant rules which could not return valid programs, and my implementation of optimisation means that clingo has to enumerate all possible programs before returning a result, instead of being able to return as soon as a solution is found as would occur using a stream of rules. However, it is not clear how this would be easily implemented in ASP. \\ \\
Another potential skeleton rule generation optimisation would be to be explicit with the difference between rules that can appear in the base case and the recursive cases of generated functions. Currently, all possible rules can appear in the base case of a recursive function, even complex function calls. This means that the learning task has to evaluate a large number of redundant possibilities, reducing performance. \\ \\
To avoid this, the tool would limit the possible rules for base cases to simple arithmetic functions that are only one, potentially two operations deep. Then, the recursive cases would be limited to only expressions which contain calls to functions or in-built operations. Theoretically, this optimisation could lead to around a 50\% increase in performance on functions with two cases, one base case and one recursive case, as it would greatly reduce the time taken to enumerate the base case, which in this case is half the rules needed to be learned.

\subsection{Supporting Multiple Function Calls}
The main limitation on the constraint based learning approach is that it cannot handle multiple function calls. Usually when evaluating equality constraints, one of the two arguments is a constant, allowing the tool to calculate the value of any non-constant arguments. For example, the evaluation of constraint \lstinline{eq(mul(3, call(f, 2)), 6)} occurs with the operations %{ 
\begin{align*}
3 * f(2) &= 6 \\
f(2) &=  6 / 3 \\
f(2) &= 2
\end{align*}
However, if the tool encounters a equality constraint with a function call as both arguments, similar to \lstinline{eq(add(f(6), f(5)), 8)}, then it cannot proceed as it does not know the constant value of either function call. \\ \\%{
Ways to overcome this problem involve making assumptions about the value of either function call. As the value of constants in the tool is constrained to a small subset of integers, the range of possible values a function call can return is also constrained to this subset. Using this information, it would be possible to make assumptions about the value of either function call in the equality constraint, through use of choice rules. \\
\begin{lstlisting}
1 { value(call(f, 5), R) : const_number(R) } 1. 
1 { value(call(f, 6), R) : const_number(R) } 1. 
\end{lstlisting}  
\mbox{} \\
This rule generates a \lstinline{value(call(f, 5), R)} atom for each integer constant, then chooses each one to be tried in evaluation of the equality constraint. Using the example above, if the tool chooses \lstinline{value(call(f, 5), 3)} then it can now evaluate the constraint as follows
\begin{align*}
f(6) + f(5) &= 8 \\
f(6) &= 8 - 3 \\
f(6) &= 5
\end{align*}
However, for this to allow the constraint to hold, more restrictions are required. Both sides of the constraint have to be explored, as it could be possible for an equality constraint to hold on one side but not the other. In addition, all \lstinline{value} atoms generated have to be consistent. If for one constraint the tool chooses that \lstinline{value(call(f, 5), 3)} and another chooses \lstinline{value(call(f, 5), 5)}, then this should be unsatisfiable, as the functional nature of the target language means that a function can only return one result. \\ \\ %{
The main downside of this approach would be that it has the potential to severely reduce performance. Any additional choice rules increase the complexity of the learning, adding more combinations of rules to be learned. Perhaps it is good enough to accept that the trade off to the increased performance of the constraint based approach is reduced expressivity. 

\subsection{Parallel Learning}
One way to handle having multiple approaches would be to not completely replace the initial approach, and instead run them both in parallel. Through the user interface, when a new learning task is started it would be possible to start clingo runs of both approaches. Then, whichever run returns first is displayed as the learned program. While this would usually be the constraint based approach, occasionally it will fail as that approach is less expressive. This restriction does not apply to the interpreted approach, which would take longer but could find solutions the first approach misses. \\ \\
This would be implemented as part of the UI through use of Akka actors. As my UI already makes use of these to allow the learning to happen in the background, it would not be difficult to run another actor, then terminate both once one returns a result.

\pagebreak
%\renewcommand\bibname{{References}}
%\bibliography{References}
%\bibliographystyle{plain}