\chapter{Learning from Examples}

This chapter will detail how I use the interpreter discussed in chapter 3 to perform learning. The tool enumerates all possible rules, then chooses the ones which cover all of the examples.

\section{Additional Rules}
Together with my interpreter, I need the following addition predicates and rules :
\begin{itemize}
\item \lstinline{example(Input, Output)} : This predicate represents an Input / Output pair.
\item \lstinline{choose(R, N)} and \lstinline{choose_where(N)} : These predicates represent the choice of a rule, with depth R, that covers all of the examples.
\item \lstinline{input(In) :- example(In, _).} : This rule generates the initial inputs for my interpreter.
\item \lstinline{:- not output(In, Out), example(In, Out).}  : This constraint represents that you cannot have an example which does not produce a matching output. In other words, this rule rule will removes rules which do not cover the examples.
\end{itemize}
%\end{lstlisting}

\section{Skeleton Rules}
To know what rules are possible to learn, I enumerate all possible combinations of rules, to provide a set for the learning task to choose from. While it may seem that the possible search space is very large, this is only partly true, due to the optimisations allowed by use of \lstinline{where} clauses. \\ \\ %{
Each skeleton rule has one of the following formats: \\

\lstinline{rule(R, F, Args, Expr) :- input(call(f, Args)), choose(R, N).} %{
\mbox{} \\

\lstinline{where(Var, Args, Expr) :- input(call(f, Args)), choose_where(N).} %{
\mbox{} \\ \\
Where \lstinline{Expr} is one of the possible rule bodies. %{

\subsection{Generating Skeleton Rules}
Initially, it was difficult to decide how to generate the skeleton rules. While it would not be difficult to naively iterate over all combinations up to a certain depth, this method is very inefficient, especially when considering more than one input argument. This is because a sizeable subset of the generated rules are redundant, having semantically equivalent bodies to other rules. For example, the rule bodies \lstinline{add(x, x)} and \lstinline{mul(x, 2)} compute the same result, but are counted as distinct and separate skeleton rules.\\ \\
To reduce the number of redundant rules, I have implemented a number of optimisations. By generating rules in a recursive manner, deeper rules are generated based off shallower ones. Then, by removing simple redundant rules when the depth is small (where they are easier to detect), rules with the same redundancy are removed from deeper iterations. \\ \\
Another optimisation is the implementation of \lstinline{where} rules. These rules limit the depth of each individual rule body to one operation deep, making enumerating over all possibilities much easier, and reducing the overall scaling of the number of rules from multiplicative to additive. %{

\subsection{Choice Rules}
To run the interpreter on all possible rule combinations, I make use of ASP choice rules. For example, the statement :

\begin{lstlisting}
1 {
choose(R, 2..5;11..16;40),
choose(R, 1;6..10, C0) : expr_const(C0)
} 1 :- num_rules(R).
\end{lstlisting}
\mbox{} \\
Represents the learning task choosing exactly one of the skeleton rules for each of the possible program rules (or recursive case). Additionally, I need a choice rule for every potential where rule : 

\begin{lstlisting}
0 {
choose_where(17..19;24..28;30),
choose_where(20..23;29, C1) : expr_const(C1)
} 1.
\end{lstlisting}
\mbox{} \\
Here, it is not guaranteed for a where rule to be chosen, as they may not all be necessary in different learning tasks. 

\subsection{Rule combinations}
The depth of the search space is reliant on three main factors : number of arguments, range of allowable constants and target language complexity. My initially small target language means that I only have to enumerate over addition, subtraction, multiplication and function calls, but as I add more expressions (i.e boolean functions), the size of the skeleton rules increases respectively. \\ \\
Similarly, the number of arguments of the target function increases as I have to enumerate all possible pairs. For example, for a simple predicate like addition I need to include : (where X, Y are arguments, C is an arbitrary learned constant, and x1 and x2 are where variables) 

\begin{multicols}{2}
\begin{itemize}
\item \lstinline{add(X, X)}
\item \lstinline{add(X, Y)}
\item \lstinline{add(X, C)}
\item \lstinline{add(X, x1)}
\item \lstinline{add(X, x2)}
\item \lstinline{add(Y, Y)}
\item \lstinline{add(Y, C)}
\item \lstinline{add(Y, x1)}
\item \lstinline{add(Y, x2)}
\item \lstinline{add(C, x1)}
\item \lstinline{add(C, x2)}
\item \lstinline{add(x1, x2)}
%\end{lstlisting}
\end{itemize}
\end{multicols}

Even in this simple example, there are 12 possible rules. As addition is commutative, I have omitted any rules where the ordering is reversed. More optimisations like this can be seen in the next section.
A full list of the skeleton rules generated for both one and two argument functions can be seen in the appendix.

\subsection{Learning Match Rules}
Until now, I have been assuming that the tool knows about the specific rule guard matching behaviour before learning takes place. While this is helpful initially, for this tool to work I need to also learn the respective match rules. Luckily, this is not particularly complicated. \\ \\
As guard rules have to return booleans, this reduces the number of operations to the integer comparators, \lstinline{equals (==)} and \lstinline{less than (<)}. I choose to omit \lstinline{greater than (>)} as I can replace all occurrences of it with \lstinline{(<)} without loss of generality.\\ \\
Using these operations, the skeleton rules for match terms are given by : \\

\begin{lstlisting}
match_guard(gcd, R, (N0, N1)) :- 
		N0 == C1, input(call(gcd, (N0, N1))), choose_match(R, 1, C1).
		
match_guard(gcd, R, (N0, N1)) :- 
		N1 == C1, input(call(gcd, (N0, N1))), choose_match(R, 2, C1).
		
match_guard(gcd, R, (N0, N1)) :- 
		N0 == N1, input(call(gcd, (N0, N1))), choose_match(R, 3).
		
match_guard(gcd, R, (N0, N1)) :- 
		N0 < N1, input(call(gcd, (N0, N1))), choose_match(R, 4).
		
match_guard(gcd, R, (N0, N1)) :- 
		N1 == N0, input(call(gcd, (N0, N1))), choose_match(R, 5).
		
match_guard(gcd, R, (N0, N1)) :- 
		N1 < N0, input(call(gcd, (N0, N1))), choose_match(R, 6).
\end{lstlisting}
\mbox{}\\
To choose these rules, I once again make use of a choice rule, which makes sure we choose exactly as many skeleton match rules as the user defines. \\

\begin{lstlisting}
1 {
choose_match(R, 3;4;5;6) ,
choose_match(R, 1;2, C0) : expr_const(C0)
} 1 :- num_match(R).
\end{lstlisting}

\section{Multiple Solutions and Optimisation}
While learning, it is not uncommon to have multiple solutions, usually due to multiple semantically equivalent base cases. I may get two answer sets as output, one having learned \lstinline{rule(1, F, 0, 1)}, and the other with \lstinline{rule(1, F, 0, 0+1)}. \\ \\
To attempt at dealing with this, I have implemented a basic optimisation system, by prioritising rules with shorter bodies. In general, rules with lower rule number are shorter, so I used the minimisation rule :

\begin{lstlisting}
#minimise [choose(1, N)=N, choose(1, N, _)=N ].
\end{lstlisting}
\mbox{}\\
Which prioritises rules with lower rule numbers.\\ \\
As a second optimisation, I wanted to prefer less complicated results, and answer sets with fewer \lstinline{where} clauses represent less complicated functions, as the depth of the function is lower. Because of this, I use the minimisation rule : \\ %{

\begin{lstlisting}
#minimise[choose_where(R)=1, choose_where(R, C)=1].
\end{lstlisting}

\section{A Worked Example : Learning GCD}
In the last chapter, I went through the steps that the interpreter takes to produce results when given Euler's algorithm to calculate the greatest common divisor. In this section, I will detail how I then learn that program. As a reminder, this function is defined in Haskell as :

\begin{lstlisting}
gcd x y
	| x == y = x
	| x > y	 = gcd (x - y) y
	| x < y	 = gcd x (y - x)
\end{lstlisting}
\mbox{}\\
As input to the learning task I need to specify some examples. I initially chose the examples \\

\lstinputlisting[language=Prolog, firstline=32, lastline=36]{../ASP/eq/gcd.lp}
\begin{lstlisting}
example(call(gcd, (1, 1)), 1).
example(call(gcd, (2, 1)), 1).
example(call(gcd, (4, 3)), 1).
example(call(gcd, (3, 6)), 3).
example(call(gcd, (9, 6)), 3).
\end{lstlisting}
\mbox{}\\
Because they seem to cover all of the cases I need to learn. In addition to this, I will need to enumerate all of the possible \lstinline{rule} and \lstinline{where} bodies as part of the skeleton rules. I will not list the entire set of skeleton rules here, but instead highlight some which will be in use later : \\

\begin{lstlisting}
rule(R, gcd, (N0, N1), N0) :- input(call(gcd, (N0, N1))), choose(R, 1).
rule(R, gcd, (N0, N1), N1) :- input(call(gcd, (N0, N1))), choose(R, 2).
rule(R, gcd, (N0, N1), sub(N0, N1)) :- input(call(gcd, (N0, N1))), choose(R, 23).
rule(R, gcd, (N0, N1), call(gcd, (N1, x0))) :- input(call(gcd, (N0, N1))), choose(R, 30).
rule(R, gcd, (N0, N1), call(gcd, (x1, N0))) :- input(call(gcd, (N0, N1))), choose(R, 81).

where(x0, (N0, N1), sub(N0, N1)) :- input(call(gcd, (N0, N1))), choose_where(53).
where(x1, (N0, N1), sub(N1, N0)) :- input(call(gcd, (N0, N1))), choose_where(77).

match_guard(gcd, R, (N0, N1)) :- N0 == N1, input(call(gcd, (N0, N1))), choose_match(R, 3).
match_guard(gcd, R, (N0, N1)) :- N0 < N1, input(call(gcd, (N0, N1))), choose_match(R, 4).
match_guard(gcd, R, (N0, N1)) :- N1 < N0, input(call(gcd, (N0, N1))), choose_match(R, 6).
\end{lstlisting}
\mbox{}\\
After running the learning task with these inputs, I get as output the terms : \\

\begin{multicols}{2}
\begin{lstlisting}
choose(1, 1).
choose(2, 23).
choose(3, 2).
choose_match(1, 4).
choose_match(2, 6).
choose_match(3, 3).
\end{lstlisting}
\end{multicols}
\mbox{}\\
These rules represent the learned Haskell program: \\

\begin{lstlisting}
gcd x y
	| x == y = y
	| x > y  = x - y
	| x < y  = x
\end{lstlisting}
\mbox{}\\
What is interesting is that this is not a correct implementation of Euler's algorithm! However, this result is still returned as it covers all of the examples. For example, the example \lstinline{example(call(gcd, (9, 6)), 3)} generates a \lstinline{input(call(gcd, (9, 6)))} term. This term is then ran through the interpreter with the chose rules, and returns \lstinline{output(call(gcd, (9, 6)), 3)}. As this does not contradict the example, those rules are valid. \\ \\%{
As an attempt to fix this problem, I can add some extra examples. By adding these two examples :\\

\lstinputlisting[language=Prolog, firstline=37, lastline=38]{../ASP/eq/gcd.lp}
\begin{lstlisting}
example(call(gcd, (4, 7)), 1).
example(call(gcd, (9, 3)), 3).
\end{lstlisting}
\mbox{}\\
These examples are cases where the previous result would not work, and help generalise the target function. \\ \\
After running these examples as input, I get the following terms as a result : \\

\begin{lstlisting}
choose(1, 1).
choose(2, 81).
choose(3, 30).
choose_where(77).
choose_where(53).
choose_match(1, 3).
choose_match(2, 4).
choose_match(3, 6).
\end{lstlisting}
\mbox{} \\
These terms translate to the haskell program :

\begin{lstlisting}
gcd x y
	| x == y = x
	| x > y  = gcd y x0
	| x < y  = gcd x1 x
	
	where x0 = x - y
	where x1 = y - x
\end{lstlisting}
\mbox{}\\
As required.

\section{Performance Issues}
Unfortunately, this learning task suffers from extreme difficulty scaling. While the number of skeleton rules is relatively small, the problem comes when considering combinations of \lstinline{rule} and \lstinline{where} terms.\\ \\ %Add specific maths here?
For the example described above, the output ground program is 366879 lines long, and takes over 500 seconds to run, which is far too long, especially for someone using a front-end user interface.

\subsection{Potential Optimisations}
However, there are many potential optimisations I could make here, to increase performance. The first involves limiting the range of available constants. As default, I restricted integer constants to 0 - 10, defining this using the term \lstinline{const_number(0..10)}. However, it may be more efficient to limit the maximum integer size to the largest constant found in the examples. This would allow for great increase in performance, especially if working on expressions such as lists, where the value inside of the list usually does not matter. \\ \\ %{
However, this would limit internal values inside the program. If a program requires an internal variable whose value is greater than any of the examples, then my tool would fail. But, it is important to consider if there are very many programs in which this would occur, especially considering my limited language bias. \\ \\
Other potential optimisations include
\begin{itemize}
\item Removing redundant skeleton rules
\item Using clingo's built in arithmetic for simple expressions.
\item Finding the optimal number of potential where rules.
\end{itemize}
In the end, I did not choose to implement any of these optimisations, and instead opted to implement an entirely new approach, as detailed in the next chapter.

\pagebreak
%\renewcommand\bibname{{References}}
%\bibliography{References}
%\bibliographystyle{plain}