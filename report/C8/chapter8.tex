\chapter{Learning from Examples}

This chapter details how we extend the interpreter discussed in chapter 3 to perform learning. The tool enumerates all possible rules, then chooses a subset which correctly evaluates all of the examples.

\section{The Learning Task}
Formally, the learning task described in this section is defined as : \\ \\
Given a set of skeleton rules which describe all potential learned programs \lstinline!SK!, and a set of examples in the form of Input / Output pairs \lstinline!E!, the Learning Task returns a subset of the skeleton rules which covers all examples. This subset corresponds to the a subset of the Answer Set of \lstinline!SK! $\cup$ \lstinline!E! $\cup$ \lstinline!I!, where \lstinline!I! is the interpreter described in Chapter 3, with some minor adjustments.\\ \\
An example is covered by some rules if, when the inputs of that example are ran by the interpreter, it returns the same result specified by the example.

\section{Skeleton Rules}
To know what rules are possible to learn, I enumerate all possible combinations of rules, to provide a set for the learning task to choose from. Based on the notion of skeleton rules from the ASP learning algorithm ASPAL, each skeleton rule is a template for a possible rule, with its constants replaced with variables. These variables are then ground when they appear in the choice rule. This replacement shifts the task of finding ground rules to the ASP solver.\\ \\
Each skeleton rule has one of the following formats: \\

\begin{lstlisting}
rule(R, F, Args, Expr) :- input(call(f, Args)), choose(R, N, C1, ..., CN).
\end{lstlisting}
\mbox{} \\

\begin{lstlisting}
where(Var, Args, Expr) :- input(call(f, Args)), choose_where(N, C1, ..., CN).
\end{lstlisting}
\mbox{} \\ \\
Where \lstinline!R! is the line number of the rule to be learned, \lstinline!N! is a unique identifier for the skeleton rule, and \lstinline!Expr! is one of the possible rule bodies, with any constants replaced with variables \lstinline!C1, ..., CN!. Each skeleton rule contains a corresponding \lstinline!choose! term, which represents the skeleton rule. \\ \\
For example, one such skeleton rule is \\

\begin{lstlisting}
rule(R, F, X, mul(X, C1)) :- input(call(f, X)), choose(R, 12, C1).
\end{lstlisting}
\mbox{}\\
Here, \lstinline!C1! represents the range of possible integer constants that can be learned by my tool. When this range is limited to the constants \lstinline!0! and \lstinline!1!, this skeleton rule represents the two rules : \\

\begin{lstlisting}
rule(R, F, X, mul(X, 0)) :- input(call(f, X)).
rule(R, F, X, mul(X, 1)) :- input(call(f, X)).
\end{lstlisting}
\mbox{}\\
The choice of these rules are represented by \lstinline!choose(R, 12, 0)! and \lstinline!choose(R, 12, 1)! respectively. \\ \\
While it may seem that the possible search space is very large, this is only partly true, due to the optimisations allowed by use of \lstinline!where! rules.

\subsection{Choice Rules}
To run the interpreter on all possible skeleton rule combinations, I make use of ASP choice rules. For example, the statement :\\

\begin{lstlisting}
1 {
choose(R, 2..5;11..16;40),
choose(R, 1;6..10, C0) : expr_const(C0)
} 1 :- num_rules(R).
\end{lstlisting}
\mbox{} \\
Represents the learning task choosing exactly one of the skeleton rules for each of the possible program rules (or recursive cases). If a rule has constants then these are defined with the predicate \lstinline!expr_const!, meaning that a \lstinline!choose! term appears for each constant. \\ \\
Additionally, a choice rule is needed for every potential where rule : \\

\begin{lstlisting}
0 {
choose_where(17..19;24..28;30),
choose_where(20..23;29, C1) : expr_const(C1)
} 1.
\end{lstlisting}
\mbox{} \\
Here, it is not guaranteed for a where rule to be chosen, as they may not all be necessary in different learning tasks. 

\section{Additional Rules}
Together with the terms defined for the interpreter, the following addition predicates and rules are needed.
\begin{center}
\begin{tabular}{| c | m{0.4\linewidth} |}
\hline
\textbf{ASP Term} & \textbf{Semantics} \\
\hline
\lstinline!example(Input, Output)!
&
\mbox{}\newline
An Input / Output pair, taken as input to the learning task.\newline
\\
\hline
\lstinline!expr_const(X)!
&
\mbox{}\newline
A possible constant to be learned. Varies per task, but is usually limited to 0, 1 and 2.\newline
\\
\hline
\lstinline!num_rules(R)!
&
\mbox{}\newline
The number of rules to be learned. Corresponds to the number of recursive cases in the learned function.\newline
\\
\hline
\lstinline!input(In) :- example(In, _).!
&
\mbox{}\newline
This rule generates the initial inputs for the interpreter.\newline
\\
\hline
\lstinline!:- not output(In, Out), example(In, Out).!
&
\mbox{}\newline
This constraint represents that you cannot have an example which does not produce a matching output. In other words, this rule rule will removes rules which do not cover the examples. \newline
\\
\hline
\end{tabular}
\end{center}

\subsection{Generating Skeleton Rules}
Initially, it was difficult to decide how to generate the skeleton rules. While it would not be difficult to naively iterate over all combinations up to a certain depth, this method is very inefficient, especially when considering more than one input argument. This is because a sizeable subset of the generated rules are redundant, having semantically equivalent bodies to other rules. For example, the rule bodies \lstinline{add(x, x)} and \lstinline{mul(x, 2)} compute the same result, but are counted as distinct and separate skeleton rules.\\ \\
To reduce the number of redundant rules, a number of optimisations have been implemented. By generating rules in a recursive manner, deeper rules are generated based off shallower ones. Then, by removing simple redundant rules when the depth is small (where they are easier to detect), rules with the same redundancy are removed from deeper iterations. \\ \\
Another optimisation is the implementation of \lstinline{where} rules. These rules limit the depth of each individual rule body to one operation deep, making enumerating over all possibilities much easier, and reducing the overall scaling of the number of rules from multiplicative to additive. %{

\subsection{Rule combinations}
The depth of the search space is reliant on three main factors : number of arguments, range of allowable constants and target language complexity. My initially small target language means that the learned only has to enumerate over addition, subtraction, multiplication and function calls, but as more operations are added (i.e boolean functions), the size of the skeleton rules increases respectively. \\ \\
Similarly, the number of arguments of the target function increases as I have to enumerate all possible pairs. For example, for a simple predicate like addition, it needs to include : (where X, Y are arguments, C is an arbitrary learned constant, and x1 and x2 are where variables) 

\begin{multicols}{2}
\begin{itemize}
\item \lstinline{add(X, X)}
\item \lstinline{add(X, Y)}
\item \lstinline{add(X, C)}
\item \lstinline{add(X, x1)}
\item \lstinline{add(X, x2)}
\item \lstinline{add(Y, Y)}
\item \lstinline{add(Y, C)}
\item \lstinline{add(Y, x1)}
\item \lstinline{add(Y, x2)}
\item \lstinline{add(C, x1)}
\item \lstinline{add(C, x2)}
\item \lstinline{add(x1, x2)}
%\end{lstlisting}
\end{itemize}
\end{multicols}

Even in this simple example, there are 12 possible rules. As addition is commutative, any rules where the ordering is reversed are omitted. More optimisations like this can be seen in the next section.
A full list of the skeleton rules generated for both one and two argument functions can be seen in the appendix.

\subsection{Learning Match Rules}
Until now, we have been assuming that the tool knows about the specific rule guard matching behaviour before learning takes place. While this is helpful initially, for this tool to work it needs to also learn the respective match rules. Luckily, this is not particularly complicated. \\ \\
As guard rules have to return booleans, this reduces the number of operations to the integer comparators, \lstinline{equals (==)} and \lstinline{less than (<)}. I choose to omit \lstinline{greater than (>)} as all occurrences of it can be replaced with \lstinline{(<)} without loss of generality.\\ \\
Using these operations, the skeleton rules for match terms are given by : \\

\begin{lstlisting}
match_guard(gcd, R, (N0, N1)) :- 
		N0 == C1, input(call(gcd, (N0, N1))), choose_match(R, 1, C1).
		
match_guard(gcd, R, (N0, N1)) :- 
		N1 == C1, input(call(gcd, (N0, N1))), choose_match(R, 2, C1).
		
match_guard(gcd, R, (N0, N1)) :- 
		N0 == N1, input(call(gcd, (N0, N1))), choose_match(R, 3).
		
match_guard(gcd, R, (N0, N1)) :- 
		N0 < N1, input(call(gcd, (N0, N1))), choose_match(R, 4).
		
match_guard(gcd, R, (N0, N1)) :- 
		N1 == N0, input(call(gcd, (N0, N1))), choose_match(R, 5).
		
match_guard(gcd, R, (N0, N1)) :- 
		N1 < N0, input(call(gcd, (N0, N1))), choose_match(R, 6).
\end{lstlisting}
\mbox{}\\
To choose these rules, the learner once again uses a choice rule, which makes sure it chooses exactly as many skeleton match rules as the user defines. \\

\begin{lstlisting}
1 {
choose_match(R, 3;4;5;6) ,
choose_match(R, 1;2, C0) : expr_const(C0)
} 1 :- num_match(R).
\end{lstlisting}

\section{Solutions}
The solution of the learning task is a subset of the skeleton rules, corresponding to a subset of the Answer Set of \lstinline!SK! $\cup$ \lstinline!E! $cup$ \lstinline!I!, where \lstinline!SK! is the set of skeleton rules, \lstinline!E! is the set of input examples and \lstinline!I! is the interpreter. This Answer Set contains \lstinline!choose! and \lstinline!choose_where! atoms, specifying which of the skeleton rules are a valid solution. The solution is then the specified skeleton rules. \\ \\
For example, if the returned answer set contains the atoms \\

\begin{lstlisting}
choose(1, 3, 1).
choose(2, 19).
choose_where(34).
choose_where(45, 1).
\end{lstlisting}
\mbox{}\\
Then these atoms correspond to the following ground skeleton rules.\\

\begin{lstlisting}
rule(1, f, X, 1) :- input(call(f, X)), choose(1, 3, 1).
rule(2, f, X, mul(X, x1)) :- input(call(f, X)), choose(2, 19).
where(x1, X, call(f, x2)) :- input(call(f, X)), choose_where(34).
where(x2, X, sub(X, 1)) :- input(call(f, X)), choose_where(45, 1).
\end{lstlisting}

\subsection{Multiple Solutions and Optimisation}
While learning, it is not uncommon to have multiple solutions, usually due to multiple semantically equivalent base cases. There may be two answer sets as output, one having learned \lstinline{rule(1, F, 0, 1)}, and the other with \lstinline{rule(1, F, 0, 0+1)}. \\ \\
To deal with this, a basic optimisation system was needed, by prioritising rules with shorter bodies. Rules with shorter bodies are more optimal because they are simpler, requiring fewer steps to execute. Due to the way I have ordered my skeleton rules,  rules with lower rule number are shorter, so I used the minimisation rule :

\begin{lstlisting}
#minimise [choose(1, N)=N, choose(1, N, _)=N ].
\end{lstlisting}
\mbox{}\\
As a second optimisation, less complicated results are preferred. Answer sets with fewer \lstinline{where} clauses represent less complicated functions, as the depth of the function is lower. Because of this, I use the minimisation rule : \\ %{

\begin{lstlisting}
#minimise[choose_where(R)=1, choose_where(R, C)=1].
\end{lstlisting}
\mbox{}\\
For instance, two potential Haskell functions the tool could learn are : \\

\begin{lstlisting}
f n
| n == 0 = n * 0
| otherwise = n * x1
		where x1 = f x2
			where x2 = n - x3
				where x3 = 1 
				
f n
| n == 0 = 0
| otherwise = n * x1
		where x1 = f x2
			where x2 = n - 1
\end{lstlisting}
\mbox{}\\
Here, the second function is considered more optimal as it has fewer lines those lines have overall shorter length.

\section{A Worked Example : Learning GCD}
In the last chapter, I went through the steps that the interpreter takes to produce results when given Euler's algorithm to calculate the greatest common divisor. In this section, I will detail how the tool then learns that program. As a reminder, this function is defined in Haskell as :

\begin{lstlisting}
gcd x y
	| x == y = x
	| x > y	 = gcd (x - y) y
	| x < y	 = gcd x (y - x)
\end{lstlisting}
\mbox{}\\
As input to the learning task some examples need to be specified. I initially choose the examples \\

\lstinputlisting[language=Prolog, firstline=32, lastline=36]{../ASP/eq/gcd.lp}
\begin{lstlisting}
example(call(gcd, (1, 1)), 1).
example(call(gcd, (2, 1)), 1).
example(call(gcd, (4, 3)), 1).
example(call(gcd, (3, 6)), 3).
example(call(gcd, (9, 6)), 3).
\end{lstlisting}
\mbox{}\\
Because they seem to cover all of the cases the tool needs to learn. In addition to this, it will need to enumerate all of the possible \lstinline{rule} and \lstinline{where} bodies as part of the skeleton rules. The entire set of skeleton rules will not be listed here here, but instead some are highlighted which will be in use later : \\

\begin{lstlisting}
rule(R, gcd, (N0, N1), N0) :- input(call(gcd, (N0, N1))), choose(R, 1).
rule(R, gcd, (N0, N1), N1) :- input(call(gcd, (N0, N1))), choose(R, 2).
rule(R, gcd, (N0, N1), sub(N0, N1)) :- input(call(gcd, (N0, N1))), choose(R, 23).
rule(R, gcd, (N0, N1), call(gcd, (N1, x0))) :- input(call(gcd, (N0, N1))), choose(R, 30).
rule(R, gcd, (N0, N1), call(gcd, (x1, N0))) :- input(call(gcd, (N0, N1))), choose(R, 81).

where(x0, (N0, N1), sub(N0, N1)) :- input(call(gcd, (N0, N1))), choose_where(53).
where(x1, (N0, N1), sub(N1, N0)) :- input(call(gcd, (N0, N1))), choose_where(77).

match_guard(gcd, R, (N0, N1)) :- N0 == N1, input(call(gcd, (N0, N1))), choose_match(R, 3).
match_guard(gcd, R, (N0, N1)) :- N0 < N1, input(call(gcd, (N0, N1))), choose_match(R, 4).
match_guard(gcd, R, (N0, N1)) :- N1 < N0, input(call(gcd, (N0, N1))), choose_match(R, 6).
\end{lstlisting}
\mbox{}\\
After running the learning task with these inputs, the following Answer Set is returned as output. \\

\begin{multicols}{2}
\begin{lstlisting}
choose(1, 1).
choose(2, 23).
choose(3, 2).
choose_match(1, 4).
choose_match(2, 6).
choose_match(3, 3).
\end{lstlisting}
\end{multicols}
\mbox{}\\
These rules represent the learned Haskell program: \\

\begin{lstlisting}
gcd x y
	| x == y = y
	| x > y  = x - y
	| x < y  = x
\end{lstlisting}
\mbox{}\\
What is interesting is that this is not a correct implementation of Euler's algorithm! However, this result is still returned as it covers all of the examples. For example, the example \lstinline{example(call(gcd, (9, 6)), 3)} generates a \lstinline{input(call(gcd, (9, 6)))} term. This term is then ran through the interpreter with the chose rules, and returns \lstinline{output(call(gcd, (9, 6)), 3)}. As this does not contradict the example, those rules are valid. \\ \\%{
As an attempt to fix this problem, some extra examples can be added. By adding these two examples :\\

\begin{lstlisting}
example(call(gcd, (4, 7)), 1).
example(call(gcd, (9, 3)), 3).
\end{lstlisting}
\mbox{}\\
These examples are cases where the previous result would not work, and help generalise the target function. \\ \\
After running these examples as input, the following Answer Set is returned. \\

\begin{lstlisting}
choose(1, 1).
choose(2, 81).
choose(3, 30).
choose_where(77).
choose_where(53).
choose_match(1, 3).
choose_match(2, 4).
choose_match(3, 6).
\end{lstlisting}
\mbox{} \\
These terms translate to the haskell program :

\begin{lstlisting}
gcd x y
	| x == y = x
	| x > y  = gcd y x0
	| x < y  = gcd x1 x
	
	where x0 = x - y
	where x1 = y - x
\end{lstlisting}
\mbox{}\\
As required.

\section{Performance Issues}
Unfortunately, this learning task suffers from extreme difficulty scaling. While the number of skeleton rules is relatively small, the problem comes when considering combinations of \lstinline{rule} and \lstinline{where} terms.\\ \\ %Add specific maths here?
For the example described above, the output ground program is 366879 lines long, and takes over 400 seconds to run, which is far too long, especially for someone using a front-end user interface.

\subsection{Potential Optimisations}
However, there are many potential optimisations I could make here, to increase performance. The first involves limiting the range of available constants. As default, I restricted integer constants to 0 - 10, defining this using the term \lstinline{const_number(0..10)}. However, it may be more efficient to limit the maximum integer size to the largest constant found in the examples. This would allow for great increase in performance, especially if working on expressions such as lists, where the value inside of the list usually does not matter. \\ \\ %{
However, this would limit internal values inside the program. If a program requires an internal variable whose value is greater than any of the examples, then my tool would fail. But, it is important to consider if there are very many programs in which this would occur, especially considering my limited language bias. \\ \\
Other potential optimisations include
\begin{itemize}
\item Removing redundant skeleton rules
\item Using clingo's built in arithmetic for simple expressions.
\item Finding the optimal number of potential where rules.
\end{itemize}
In the end, I did not choose to implement any of these optimisations, and instead opted to implement an entirely new approach, as detailed in the next chapter.

\pagebreak
%\renewcommand\bibname{{References}}
%\bibliography{References}
%\bibliographystyle{plain}