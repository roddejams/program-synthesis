\chapter{A Second Approach : Constraint Based Learning}

As seen in the previous chapter, my initial approach suffered from significant difficulty scaling. Even simple two argument functions took upwards of 130 seconds to complete. This was mainly due to the sheer size of the ground learning task, and the complexity from combinations of rule and where predicates.

\section{Top Down Vs. Bottom Up}
The main issue with the interpreted approach is that it is bottom-up. To learn the output of a rule, we must first iterate down the expression tree, calculate the value of each simple sub-expression, then iterate back up the tree combining values until we know a value for the entire rule body. My second approach overcomes this issue by implementing a top down approach. \\ \\
The idea behind this new approach is simple. By maintaining an "equality constraint", I keep track of what each expression is supposed to be equal to (as defined by the input examples). Then, as the program iterates down the expression tree, it fails if it ever finds some easily provable equality failure, i.e 1 == 2. \\ \\
For example, if I know that \lstinline{call(f, 2) == 5} and that the body of function f is \lstinline{2*X + 1}, then I can deduce that 
\begin{align*}
(2*2) + 1 &== 5 \\
(2*2) &== 4 \\
2 &== 2
\end{align*}
So there are no contradictions. \\ \\
However, if instead we have an example stating that \lstinline{call(f, 2) == 6}, with the same function, then instead we get %{
\begin{align*}
(2*2) + 1 &== 6 \\
(2*2) &== 5 \\
\end{align*}
Which fails because 5 is not a multiple of 2.

\subsection{Dealing with termination}
One issue with this new approach is that it does not automatically handle programs which do not terminate. Whilst these programs do not stop clingo from finding a solution, they are incorrect as the constraint is never met. \\ \\
To deal with this, I have to check input examples for termination. While this is typically an undecidable problem, for my small target language it is decidable and computable. To represent this in ASP, I first needed a way to represent the next step of execution of an expression. For example, I can say that \lstinline{next_step(add(1, mul(2, 3)), mul(2, 3))}, meaning that I next evaluate mul(2, 3). \\ \\%{
I then define termination as :
\begin{itemize}
\item If an expression is simple, containing only constants, then it terminates.
\item If the next step of an expression terminates, then that expression also terminates.
\end{itemize}

This approach is fairly efficient as it as another top down approach, and generates a similar number of rules in the ground program as the constraint checking rules.

\section{ASP Representation}
I represent this approach using the following ASP predicates.

\begin{itemize}
\item \lstinline{eq(Expr, Val)} represents an equality. Expr, when evaluated, should equal Val.
\item \lstinline{is_call(call(F, Expr)} represents if a function is called. Used to generate more ground skeleton rules.
\item \lstinline{terminates(Expr)} represents if an expression terminates.
\item \lstinline{next_step(A, B)} represents that B is the expression executed after executing A.
\end{itemize}
%\end{lstlisting}
Now, I use these predicates in the following rules. \\

\lstinputlisting[language=Prolog, firstline=35, lastline=35]{../ASP/eq/eq_rules.lp} 
\mbox{} \\
This rule generates initial equality constraints given by the examples.  \\

\lstinputlisting[language=Prolog, firstline=36, lastline=36]{../ASP/eq/eq_rules.lp} 
\mbox{} \\
This rule constrains equality on constants. The tool should fail if two different constants are equal. \\

\lstinputlisting[language=Prolog, firstline=37, lastline=40]{../ASP/eq/eq_rules.lp} 
\mbox{} \\
These constraints handle edge cases when dealing with multiplication. If multiplying any expression by 0, then it should be equal to 0, and if multiplying two things together, then the answer should be a multiple of them both. \\

\lstinputlisting[language=Prolog, firstline=43, lastline=43]{../ASP/eq/eq_rules.lp} 
\mbox{} \\
This constraint handles termination. The tool fails if an input example does not terminate.\\

\lstinputlisting[language=Prolog, firstline=45, lastline=45]{../ASP/eq/eq_rules.lp} 
\mbox{} \\
This rule generates \lstinline{is_call} predicates, which are used to generate more ground instances of skeleton rules. \\ %{

\lstinputlisting[language=Prolog, firstline=47, lastline=47]{../ASP/eq/eq_rules.lp} 
\mbox{} \\
This rule handles propagation of equality constraints through function calls. If a called function is equal to some value, then the body of the function (with correct arguments) is also equal to that value. \\

\lstinputlisting[language=Prolog, firstline=48, lastline=53]{../ASP/eq/eq_rules.lp} 
\mbox{} \\
These rule specify generation of equality predicates with arithmetic, through use of the opposite operations. Addition terms in the head of the rule are necessary to handle edge cases such as division by zero. \\

\lstinputlisting[language=Prolog, firstline=59, lastline=60]{../ASP/eq/eq_rules.lp} 
\mbox{} \\
These rules define termination, as described above. If an expression is a constant, then it terminates, or if the next step of an expression terminates then that expression also terminates. \\

\lstinputlisting[language=Prolog, firstline=62, lastline=62]{../ASP/eq/eq_rules.lp} 
\mbox{} \\
This rule generates the initial \lstinline{next_step} predicates. If there exists an example input, then it is the next step of some arbitrary term \lstinline{e}. \\

\lstinputlisting[language=Prolog, firstline=73, lastline=73]{../ASP/eq/eq_rules.lp} 
\mbox{} \\
This rule exists to reduce the grounding. If any expression is a next step, we generate an \lstinline{is_next_step} predicate, used in the remaining \lstinline{next_step} rules.

\lstinputlisting[language=Prolog, firstline=63, lastline=63]{../ASP/eq/eq_rules.lp} 
\mbox{} \\
This rule handles the next step of function calls. The next step of a function call is the body of that function.

\lstinputlisting[language=Prolog, firstline=64, lastline=69]{../ASP/eq/eq_rules.lp} 
\mbox{} \\
These rules generate \lstinline{next_step} predicates for arithmetic. The next step of an arithmetic expression is the argument that is not constant. %{

\section{Learning}
The actual learning task operates in a similar way to my initial approach. By enumerating all possible rule bodies, I can use a choice rule to try generating answer sets with each one, keeping the answer sets which are satisfiable. However, because I am no longer using \lstinline{where} clauses, the skeleton rules now contain full bodies. This means that unfortunately the number of skeleton rules becomes large, numbering in the thousands for even simple tasks. To avoid this scaling poorly once again, I decided to implement a number of simple optimisations.%{

\subsection{Using inbuilt arithmetic}
As part of my first approach, I decided to represent arithmetic with my own predicates \lstinline{add(A, B)}, \lstinline{sub(A, B)} and \lstinline{mul(A, B)} because I needed to evaluate expressions that the grounder would not be able to compute (i.e the value of function calls or where variables). \\ \\ %{
However, in my new approach I decided to make partial use of the clingo inbuilt arithmetic. If inside function arguments, or the rule body has no function calls at all, then I know all sub expressions will be arithmetic and I make use of the simple \lstinline{+, -} and \lstinline{*}. \\ \\
The main advantage of this approach is that it vastly reduces the ground output. As the inbuilt operations are computed by the grounder, if two different expressions compute the same output number, then they are not repeated in the ground output. For example, \lstinline{X + X} and \lstinline{2 * X} are semantically equivalent, so only produce one output rule.

\section{A Worked Example : Greatest common divisor}
Once again I will use Euler's algorithm for the Greatest Common Divisor to illustrate this new approach. Because my tool still does not have modulo as part of its target language, I will still attempt to learn the simplified definition :

\lstinputlisting[language=Haskell, firstline=2, lastline=5]{../ASP/eq/gcd.lp} 
\mbox{} \\
As input, I will be using the following examples : \\

\lstinputlisting[language=Prolog, firstline=20, lastline=25]{../ASP/eq/gcd.lp} 
\mbox{} \\
Which cover both cases $(X < Y)$ and $(X > Y)$ while also not being too simplistic. \\ \\
After running the learning task, the resulting Answer Set contains the terms: \\

\begin{lstlisting}
choose(1,2).
choose(2,150).
choose(3,190).
\end{lstlisting}
\mbox{} \\
These terms correspond to the following skeleton rules : \\

\begin{lstlisting}
rule(R, gcd, (N0, N1), N0) :- is_call(call(gcd, (N0, N1))), choose(R, 2). 
rule(R, gcd, (N0, N1), call(gcd, ((N0 - N1), N1))) :- is_call(call(gcd, (N0, N1))), choose(R, 150).
rule(R, gcd, (N0, N1), call(gcd, (N1, N0))) :- is_call(call(gcd, (N0, N1))), choose(R, 190).
\end{lstlisting}
\mbox{} \\
Which then corresponds to the Haskell program :

\begin{lstlisting}
gcd x y
	| x == y = x
	| x > y	 = gcd (x - y) y
	| x < y	 = gcd y x
\end{lstlisting}
\mbox{} \\
What is interesting is that whilst it has not learned the exact target program, the learned program is still correct. This is due to the optimisations I have implemented preferring rules with shorter bodies.\\ \\
To see why I get this result, it is useful to look at the corresponding \lstinline{eq} predicates for each example. \\ \\%{
The example \lstinline{example(call(gcd,(8,12)),4)} produces the terms :\\ %{
\begin{multicols}{2}
\begin{lstlisting}
eq(call(gcd,(8,12)),4). 
eq(call(gcd,(12,8)),4).
eq(call(gcd,(4,8)),4).
eq(call(gcd,(8,4)),4).
eq(call(gcd,(4,4)),4).
eq(4, 4).
next_step(e,call(gcd,(8,12))).
next_step(call(gcd,(8,12)),call(gcd,(12,8))).
next_step(call(gcd,(12,8)),call(gcd,(4,8))).
next_step(call(gcd,(4,8)),call(gcd,(8,4))).
next_step(call(gcd,(8,4)),call(gcd,(4,4))).
next_step(call(gcd,(4,4)),4).
\end{lstlisting}
\end{multicols}
\mbox{}\\
Because these terms are all created without constraints failing, the respective rule bodies are returned as a solution.

\section{Performance}
While this new approach does perform better than the old one, it still suffers from a lot of the same issues limiting performance. \\ \\
Because of the large number of combinations of skeleton rules, they can still grow very large very quickly, lowering the performance of my tool. In addition, expanding the language bias to different types also has an adverse effect on this.

\subsection{Reducing the language bias}
As a way to deal with the explosive expansion of skeleton rules, I decided to implement a way to help contain this by artificially limiting the language bias. \\ \\
By removing operations from the bias that I know will not be used in the output functions, I can remove a large number of skeleton rules that will have no effect on the output of the learning task. In a similar way, I can limit learning to only tail recursive programs, meaning that my skeleton rules only use inbuilt arithmetic operations, further increasing performance. \\ \\
Of course, this method for improving performance is limited by the knowledge of the user. If they have no idea what the output function will look like, this is completely unhelpful. 

\pagebreak
%\renewcommand\bibname{{References}}
%\bibliography{References}
%\bibliographystyle{plain}